---
title: "Quarto_Learning_Playground"
format: html
editor: visual
---

# (15/04) Manipuler des données spatiales avec sf et faire des facettes avec ggplot2

On va créer un petit dataset de 6 points fictifs dans deux "sources", avec leurs coordonnées.

```{r}
library(sf)
library(ggplot2)
library(dplyr)
```

## 1 : Créer des données spatiales simples

### Créer un petit jeu de données

```{r}
df <- data.frame(
  id = 1:6,
  lon = c(2.35, 2.36, 2.37, -1.68, -1.67, -1.69),  # Paris et Rennes
  lat = c(48.85, 48.86, 48.87, 48.11, 48.12, 48.13),
  source = c("atlas", "atlas", "atlas", "steli", "steli", "steli")
)
```

### Transformer en objet spatial

```{r}
df_sf <- st_as_sf(df, coords = c("lon", "lat"), crs = 4326)  # EPSG 4326 = WGS84
```

Maintenant `df_sf` est un objet spatial avec des points géographiques

## 2 : Visualiser les points avec ggplot2

```{r}
ggplot() +
  geom_sf(data = df_sf, aes(color = source), size = 3) +
  theme_minimal()
```

J'obtiens une carte simple avec des points colorés selon la source

## 3 : Ajouter des facettes

Je sépare les données par source, avec une facette par "atlas" et une par "steli".

```{r}
ggplot() +
  geom_sf(data = df_sf, aes(color = source), size = 3) +
  facet_wrap(vars(source)) +
  theme_minimal()
```

Résultat : deux sous-cartes côte à côte, une pour chaque source

## 4 : Changer de projection

Je transforme les données vers une projection en mètres.

```{r}
df_sf_proj <- st_transform(df_sf, 3035)

# Même graphe, mais avec projection européenne
ggplot() +
  geom_sf(data = df_sf_proj, aes(color = source), size = 3) +
  facet_wrap(vars(source)) +
  theme_minimal()
```

## 5 : Ajouter une carte en fond et simuler une grille + compter le nb de pts / case

### Ajouter une carte de fond (France)

```{r}
library(rnaturalearth)
library(rnaturalearthdata)

# Récupérer la carte de la France
fr <- ne_countries(scale = 50, country = "France", returnclass = "sf")

# La projeter comme nos données (EPSG:3035)
fr_3035 <- st_transform(fr, 3035)
df_sf_proj <- st_transform(df_sf, 3035)

# Carte avec fond
ggplot() +
  geom_sf(data = fr_3035, fill = "white", color = "black") +
  geom_sf(data = df_sf_proj, aes(color = source), size = 2) +
  facet_wrap(vars(source)) +
  coord_sf(
    xlim = c(bbox$xmin, bbox$xmax),
    ylim = c(bbox$ymin, bbox$ymax),
    expand = FALSE
  ) +
  theme_minimal()
```

### Simuler une grille spatiale

On crée une grille carrée et on compte combien de points tombent dans chaque case.

```{r}
# Créer une grille régulière qui couvre la France
grid <- st_make_grid(fr_3035, cellsize = 100000, square = TRUE)  # 100 km
grid <- st_sf(id = 1:length(grid), geometry = grid)

# Croiser la grille avec les points (spatial join)
df_joined <- st_join(df_sf_proj, grid, left = FALSE)  # left = FALSE = ne garde que les points qui tombent dans la grille

# Compter les observations par case et par source
nobs_grid <- df_joined |>
  group_by(id = id.y, source) |>
  summarize(n = n(), .groups = "drop")

# Joindre avec la grille
grid_counts <- left_join(grid, st_drop_geometry(nobs_grid), by = "id")
```

### Affichage : Carte des occurrences par cellule (avec ggplot2)

```{r}
ggplot() +
  geom_sf(data = fr_3035, fill = "white", color = "grey80") +  # fond de carte France
  geom_sf(data = grid_counts, aes(fill = n), color = NA) +     # cellules colorées
  scale_fill_viridis_c(
    trans = "log", 
    breaks = c(1, 10, 100, 1000, 10000),
    na.value = "lightgrey",
    name = "Occurrences"
  ) +
  coord_sf(xlim = c(bbox$xmin, bbox$xmax), ylim = c(bbox$ymin, bbox$ymax)) +
  theme_void(base_size = 14) +
  theme(
    legend.position = "left",
    legend.key.height = unit(1.2, "cm")
  )
```

Carte pas très intéressante avec ce jeu de données mais bon voilà.

# (16/04, 17/04) Se familiariser à data.table sur les données odonates

## Charger les données avec data.table

```{r}
library(data.table)
library(here)

# Définir le chemin du fichier
read_folder <- here("data/03_grid")
# Charger les données dans un data.table
dat <- readRDS(file.path(read_folder, "french_odonate_data.rds"))
dat <- as.data.table(dat)
# Structure des données 
str(dat)
```

## Premiers exemples pratiques

```{r}
# nb tot d'obs
dat[ , .N]  # .N = nombre de lignes
# nb d'obs par source (atlas ou steli)
dat[ , .N, by = source]
# top 10 sp les plus obervées
dat[ , .N, by = scientificName][order(-N)][1:10]
```

## Extraire l'année depuis eventDate et compter par année

```{r}
# Création d’une nouvelle colonne
dat[ , year := year(eventDate)]  
# Nombre d'observations par année
dat[ , .N, by = year][order(year)]
```

## Observations par région

```{r}
dat[ , .N, by = region][order(-N)]
```

## Observations par famille de libellules

```{r}
dat[ , .N, by = family][order(-N)]
```

## Création d’un sous-ensemble (filtrage)

```{r}
# toutes les observations de l’espèce Calopteryx splendens
dat[scientificName == "Calopteryx splendens"]
# uniquement les observations après 2010
dat[year > 2010]
```

## 4 petits challenges

### trouver les 5 genres les plus fréquents

```{r}
dat[ , .N, by = genus][order(-N)][1:5]
```

### calculer le nb moy d'obs par commune

```{r}
# Compter le nombre d'observations par commune
obs_par_commune <- dat[, .N, by = municipality][order(-N)]
# Calculer la moyenne
moyenne <- mean(obs_par_commune$N)
# Afficher les résultats
obs_par_commune
moyenne
```

```{r}
ggplot(obs_par_commune, aes(x = N)) +
  geom_histogram(bins = 100) +
  scale_x_log10() +
  theme_minimal()
summary(obs_par_commune$N)
```

### trouver combien d'sp diff ont été obs dans chaque région

```{r}
dat[, .(n_species = uniqueN(scientificName)), by = region][order(-n_species)]
```

### trouver l'année avec le plus d'obs pour chaque source

```{r}
unique(dat$source)
dat.steli <- dat[source == "STELI"]
dat.steli[ , .N, by = year][order(-N)]
dat.atlas <- dat[source == "atlas"]
dat.atlas[ , .N, by = year][order(-N)]
```

```{r}
dat[, .N, by = .(source, year)][order(-N), .SD[1], by = source]
```

# (17/04) Exercices

## Librairies et préparation des données

```{r}
library(data.table)
library(here)
library(sf)
library(ggplot2)
library(rnaturalearth)

# Définir le chemin du fichier
read_folder <- here("data/03_grid")
# Charger les données dans un data.table
dat <- readRDS(file.path(read_folder, "french_odonate_data.rds"))
dat <- as.data.table(dat)
```

```{r}
# Je dois réduire la taille des données pour simplifier les exercices
## création d’une nouvelle colonne year
dat[ , year := year(eventDate)]
## filtrage du jeu de donnée
dat_2018 <- dat[year=="2018"]
```

## Cartes simples avec sf et rnaturalearth

### Représenter toutes les données

```{r}
# Convertir en sf
dat_sf <- st_as_sf(dat_2018, coords = c("x_ETRS89_LAEA", "y_ETRS89_LAEA"))
st_crs(dat_sf) <- 3035

# Carte de la France
fr <- ne_countries(scale = 50, country = "France", returnclass = "sf")
fr_3035 <- st_transform(fr, 3035)

# Obtenir la bounding box des données
bbox <- st_bbox(dat_sf)

# Créer la carte avec zoom automatique sur les données
ggplot() +
  geom_sf(data = fr_3035, fill = "gray95") +
  geom_sf(data = dat_sf, size = 0.1, alpha = 0.5, aes(color = source)) +
  coord_sf(xlim = c(bbox$xmin, bbox$xmax),
           ylim = c(bbox$ymin, bbox$ymax)) +
  theme_minimal()
```

### Représenter les données steli seulement

```{r}
## filtrage du jeu de donnée
dat_2018steli <- dat_2018[source=="STELI"]

# Convertir en sf
dat_sf <- st_as_sf(dat_2018steli, coords = c("x_ETRS89_LAEA", "y_ETRS89_LAEA"))
st_crs(dat_sf) <- 3035

# Carte de la France
fr <- ne_countries(scale = 50, country = "France", returnclass = "sf")
fr_3035 <- st_transform(fr, 3035)

# Obtenir la bounding box des données
bbox <- st_bbox(dat_sf)

# Créer la carte avec zoom automatique sur les données
ggplot() +
  geom_sf(data = fr_3035, fill = "gray95") +
  geom_sf(data = dat_sf, size = 0.1, alpha = 0.5, aes(color = source)) +
  coord_sf(xlim = c(bbox$xmin, bbox$xmax),
           ylim = c(bbox$ymin, bbox$ymax)) +
  theme_minimal()
```

### Représenter les données steli et atlas séparément pour comparer

```{r}
# Convertir en sf
dat_sf <- st_as_sf(dat_2018, coords = c("x_ETRS89_LAEA", "y_ETRS89_LAEA"))
st_crs(dat_sf) <- 3035

# Carte de la France
fr <- ne_countries(scale = 50, country = "France", returnclass = "sf")
fr_3035 <- st_transform(fr, 3035)

# Obtenir la bounding box des données
bbox <- st_bbox(dat_sf)

# Créer la carte avec zoom automatique sur les données et un facet_wrap
ggplot() +
  geom_sf(data = fr_3035, fill = "gray95") +
  geom_sf(data = dat_sf, size = 0.1, alpha = 0.5, aes(color = source)) +
  coord_sf(xlim = c(bbox$xmin, bbox$xmax),
           ylim = c(bbox$ymin, bbox$ymax)) +
  facet_wrap(facets = vars(source))
```

## Graphiques exploratoires avec ggplot2 : Evolution temporelle de la diversité

### Nombre d’observations par an

```{r}
dat[, year := year(eventDate)]

dat_year <- dat[, .N, by = .(year, source)]

ggplot(dat_year, aes(x = year, y = N, color = source)) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  labs(title = "Évolution du nombre d’observations", y = "Nombre")
```

### Ajouter une courbe de tendance (geom_smooth)

```{r}
ggplot(dat_year, aes(x = year, y = N, color = source)) +
  geom_line() +
  geom_point() +
  geom_smooth(se = FALSE) +
  theme_minimal() +
  labs(title = "Évolution du nombre d’observations", y = "Nombre")
```

### Nombre d’espèces différentes par an

```{r}
dat[, year := year(eventDate)]

dat_year <- dat[, .(n_species = uniqueN(scientificName)), by = .(year, source)]

ggplot(dat_year, aes(x = year, y = n_species, color = source)) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  labs(title = "Évolution du nombre d'espèces différentes", y = "Nombre")
```

### Ajouter une courbe de tendance (geom_smooth)

```{r}
ggplot(dat_year, aes(x = year, y = n_species, color = source)) +
  geom_line() +
  geom_point() +
  geom_smooth(se = FALSE) +
  theme_minimal()
```

# (18/04) Exercice poussé : Quelles sont les 3 régions avec la plus grande richesse spécifique moyenne par commune ?

```{r}
# créer une table avec les communes et les régions
comm_reg <- dat[, .(municipality, region)]
# dans une autre table, compter le nombre d'espèces par commune
n_sp_comm <- dat[, .(n_species = uniqueN(scientificName)), by = municipality]
# joindre ces 2 tables par les communes, pour ajouter la région dans la table n_sp_comm
n_sp_comm_reg <- n_sp_comm[comm_reg, on = .(municipality)]
# calculer la moyenne communale par région
n_sp_comm_reg[, moy_comm := mean(n_species, na.rm = TRUE), by = region]
# agréger la table et afficher les 3 premières lignes
moy_sp_comm_by_region <- n_sp_comm_reg[, .(moy_comm = mean(n_species, na.rm = TRUE)), by = region]
moy_sp_comm_by_region[order(-moy_comm)]
```

J'obtiens la bonne solution mais je peux simplifier le code :

```{r}
# compter le nombre d'espèces par commune tout en gardant la région dans la table 
n_sp_comm_reg <- dat[, .(n_species = uniqueN(scientificName)), by = .(municipality, region)]
# calculer la moyenne par commune, par région
moy_sp_comm_by_region <- n_sp_comm_reg[, .(moy_comm = mean(n_species, na.rm = TRUE)), by = region]
# afficher les 3 plus grandes moyennes régionales
moy_sp_comm_by_region[order(-moy_comm)][1:3]
```

Cette version est plus concise mais sous-estime de moitié les moyennes régionales, ça indique une incohérence quelque part, par exemple si certaines communes sont liées à plusieurs régions, à vérifier.

## Vérification du lien communes-régions dans les données (18/04)

```{r}
# Voir combien de communes sont associées à plus d'une région
communes_probleme <- dat[, .(n_regions = uniqueN(region)), by = municipality][n_regions > 1][order(municipality)]
nrow(communes_probleme)
DT::datatable(communes_probleme)
```

Je sélectionne APREMONT (Auvergne-Rhône-Alpes) pour voir les différentes régions auxquelles elle est rattachée dans les données.

```{r}
dat[municipality == "APREMONT", .N, by = region]
```

En effet, les noms de commune ne sont pas uniques en France, je vais donc répondre à la question de l'exercice (moyennes régionales ?) mais en me servant d'identifiants uniques (ex : id_10000) plutôt que les noms de commune.

## Exercice modifié : quelles sont les 3 régions avec la plus grande richesse spécifique moyenne par cellule de 10km² ?

Je vais refaire les 2 scripts pour vérifier si j'obtiens bien le même résultat cette fois-ci

```{r}
# créer une table avec les id et les régions
id_reg <- dat[, .(id_10000, region)]
# dans une autre table, compter le nombre d'espèces par id
n_sp_id <- dat[, .(n_species = uniqueN(scientificName)), by = id_10000]
# joindre ces 2 tables par les id, pour ajouter la région dans la table
n_sp_id_reg <- n_sp_id[id_reg, on = .(id_10000)]
# calculer la moyenne id par région
n_sp_id_reg[, moy_id := mean(n_species, na.rm = TRUE), by = region]
# agréger la table et afficher les 3 premières lignes
moy_sp_id_par_region <- n_sp_id_reg[, .(moy_id = mean(n_species, na.rm = TRUE)), by = region]
moy_sp_id_par_region[order(-moy_id)][1:3]
```

```{r}
# compter le nombre d'espèces par id tout en gardant la région dans la table 
n_sp_id_reg <- dat[, .(n_species = uniqueN(scientificName)), by = .(id_10000, region)]
# calculer la moyenne par id, par région
moy_sp_id_par_region <- n_sp_id_reg[, .(moy_id = mean(n_species, na.rm = TRUE)), by = region]
# afficher les 3 plus grandes moyennes régionales
moy_sp_id_par_region[order(-moy_id)][1:3]
```

Je n'obtiens pas le même résultat, pourquoi ? En fait c'est probablement le même problème mais pour d'autres raisons, la grille 10km² ne suit pas les limites régionales donc certaines cellules doivent probablement être rattachées à plusieurs régions, ce qui crée encore une fois des incohérences selon la manière de calculer.

Bref si je veux faire des richesses spécifiques moyennes régionales je sais pas trop quelle est la meilleure méthode au final...


# En dessous = brouillon

### Agrégation des classes CLC (méthode 2 : directement sur le SpatRaster)

**Reclassification (à changer)** :

1.1 + 1.2 + 1.3 Milieux fortement anthropisés

1.4 Espaces verts urbains

2.1 + 2.2 Zones agricoles intensives

2.3 + 2.4 Zones agricoles semi-naturelles

3.1 Forêts

3.2 Milieux ouverts semi-naturels

3.3 Milieux extrêmes

4.1 + 5.1 Milieux humides continentaux

4.2 Milieux humides côtiers

5.2 Milieux marins et côtiers

C'est avec ces groupes de classes qu'on va travailler à partir de maintenant.

### Distribution attendue des groupes de classes d'occupation des sols 

Quelle proportion du territoire métropolitain est couverte par chaque classe CLC ?

Données :

-   raster CLC2018 100m https://land.copernicus.eu/en/products/corine-land-cover/clc2018 

-   shapefile frontières France métropole https://gadm.org/download_country.html

```{r}
#| eval: false
# Importer le vecteur France métropole
fr <- vect(here(gisdata_folder,"gadm41_FRA_shp", "gadm41_FRA_0.shp"))
# Importer le raster CLC2018 100m
clc <- rast(here(gisdata_folder, "180451","Results","u2018_clc2018_v2020_20u1_raster100m","u2018_clc2018_v2020_20u1_raster100m","DATA","U2018_CLC2018_V2020_20u1.tif"))
# Reprojetter le vecteur pour le superposer au raster
fr_3035 <- project(fr, crs(clc))
# Couper le raster aux frontières de la métropole
clc_fr <- crop(clc, fr_3035, mask=TRUE)
```

Maintenant qu'on a chargé le raster, il faut le reclassifier selon les groupes de classes qu'on a définit.
Pour ça il faut aller chercher une table de correspondance entre le code CLC et le code des pixels du raster.

Correspondance code CLC (3 chiffres) et grid_code (code raster, 2 chiffres) : https://clc.gios.gov.pl/doc/clc/CLC_Legend_EN.pdf

Ensuite, on définit un code pour les groupes de classes : 
-Code groupe de classe : nom groupe : codes classes correspondantes-
1 : Milieux très anthropisés : 1-9
2 : Espaces verts urbains : 10,11
3 : Zones agricoles intensives :  12-17
4 : Zones agricoles semi-naturelles : 18-22
5 : Forêts : 23-25
6 : Milieux ouverts semi-naturels : 26-29
7 : Milieux extrêmes : 30-34
8 : Milieux humides continentaux (inland wetlands + inland waters) : 35,36 + 40,41
9 : Milieux humides côtiers : 37-39
10 : Milieux marins : 42-44

```{r}
#| eval: false
# Créer une matrice de correspondance (code CLC, code groupe)
recl <- matrix(c(
  1, 1,
  2, 1,
  3, 1,
  4, 1,
  5, 1,
  6, 1,
  7, 1,
  8, 1,
  9, 1,
  10, 2,
  11, 2,
  12, 3,
  13, 3,
  14, 3,
  15, 3,
  16, 3,
  17, 3,
  18, 4,
  20, 4,
  21, 4,
  22, 4,
  23, 5,
  24, 5,
  25, 5,
  26, 6,
  27, 6,
  28, 6,
  29, 6,
  30, 7,
  31, 7,
  32, 7,
  33, 7,
  34, 7,
  35, 8,
  36, 8,
  37, 9,
  38, 9,
  39, 9,
  40, 8,
  41, 8,
  42, 10,
  43, 10,
  44, 10,
  48, NA
), ncol = 2, byrow = TRUE)
# Appliquer la reclassification sur le raster
clc_grouped <- classify(clc_fr, recl)
```

On a donc changé les valeurs des pixels du raster, passant d'un code 1-44 (CLC) à un code 1-10 (groupes). Il faut maintenant changer les labels pour qu'ils correspondent aux groupes.

```{r}
#| eval: false
# Créer une table de correspondance pour les groupes
group_labels <- data.frame(
  value = 1:10,
  group_name = c(
    "Milieux très anthropisés",
    "Espaces verts urbains",
    "Zones agricoles intensives",
    "Zones agricoles semi-naturelles",
    "Forêts",
    "Milieux ouverts semi-naturels",
    "Milieux extrêmes",
    "Milieux humides continentaux",
    "Milieux humides côtiers",
    "Milieux marins"
  )
)
# Associer ces labels au raster
levels(clc_grouped) <- group_labels
```

On peut maintenant répéter les analyses réalisées précédemment sur les classes CLC2018_landcover.

```{r}
#| eval: false
# Récupérer les valeurs de chaque pixel et compter leur fréquence
clc_values <- values(clc_grouped)
nclc_grouped <- table(clc_values)
# Calculer la proportion du territoire métropolitain couverte par chaque classe CLC
france_clc_grouped <- data.frame(
  "landcover_groups"=group_labels$group_name[match(names(nclc_grouped), group_labels$value)],
  "N_exp"=as.numeric(nclc_grouped),
  "perc_exp"=round(as.numeric(nclc_grouped)/sum(nclc_grouped)*100,3)
)
# Stocker le résultat dans un CSV
write.csv(france_clc_grouped, 
  here::here(read_folder, "france_clc2018_100m_grouped.csv"), row.names=FALSE)
```

```{r}
#| eval: false
# Afficher le résultat
clc_expected <- read.csv(file.path(read_folder, 
                          "france_clc2018_100m_grouped.csv"))
clc_expected <- as.data.table(france_clc_grouped)
clc_expected <- clc_expected[order(-N_exp)]
clc_expected |> datatable()
```

```{r}
#| eval: false
# Barplot
ggplot(clc_expected) +
  geom_col(aes(x = perc_exp, 
               y = reorder(landcover_groups, N_exp)), fill = "darkorange") +
  xlab("Part couverte du territoire de France métropole (%)") +
  scale_x_continuous(labels = label_number(big.mark = " ", decimal.mark = ",")) +
  theme_minimal(base_size = 16) +
  theme(axis.title.y = element_blank(),
        panel.grid.major.y = element_blank(),
        axis.text.y = element_text(face = "italic",
                                   size = 6))
```








### Distribution observée des classes d'occupation des sols dans les données odonate

```{r}
#| eval: false
# Création d'une nouvelle colonne de regroupement
dat[, clc_group := fcase(
  CLC2018_landcover %in% c("Continuous urban fabric", "Discontinuous urban fabric", "Industrial or commercial units", "Road and rail networks and associated land", "Port areas", "Airports", "Mineral extraction sites", "Dump sites", "Construction sites"),
  "Milieux fortement anthropisés",

  CLC2018_landcover %in% c("Green urban areas", "Sport and leisure facilities"),
  "Espaces verts urbains",

  CLC2018_landcover %in% c("Non-irrigated arable land", "Permanently irrigated land", "Ricefields", "Vineyards", "Fruit trees and berry plantations", "Olive groves"),
  "Zones agricoles intensives",

  CLC2018_landcover %in% c("Pastures", "Annual crops associated with permanent crops", "Complex cultivation patterns", "Land principally occupied by agriculture, with significant areas of natural vegetation", "Agro-forestry areas"),
  "Zones agricoles semi-naturelles",

  CLC2018_landcover %in% c("Broad-leaved forest", "Coniferous forest", "Mixed forest"),
  "Forêts",

  CLC2018_landcover %in% c("Natural grasslands", "Moors and heathland", "Sclerophyllous vegetation", "Transitional woodland-shrub"),
  "Milieux ouverts semi-naturels",
  
  CLC2018_landcover %in% c("Beaches, dunes, sands", "Bare rocks", "Sparsely vegetated areas", "Burnt areas", "Glaciers and perpetual snow"),
  "Milieux extrêmes",

  CLC2018_landcover %in% c("Inland marshes", "Peatbogs", "Water courses", "Water bodies"),
  "Milieux humides continentaux",
  
  CLC2018_landcover %in% c("Salt marshes", "Salines", "Intertidal flats"),
  "Milieux humides côtiers",

  CLC2018_landcover %in% c("Coastal lagoons", "Estuaries", "Sea and ocean"),
  "Milieux marins",

  default = "Autres"
)]
```